import json
import os
import asyncio
from channels.db import database_sync_to_async
from openai import AsyncAzureOpenAI

from interaction.models import Conversation, Message
from .models import OpenAIFile, OpenAIThread, MessageOpenAIFile
from .services import get_or_create_thread, add_message_to_thread, get_async_openai_client


async def stream_openai_assistant_response(thread_id, assistant_id, run_instructions=None):
    """
    Stream response using OpenAI assistant API.
    
    Args:
        thread_id: OpenAI Thread ID
        assistant_id: OpenAI Assistant ID
        run_instructions: Optional override instructions
        
    Returns:
        Async generator of assistant message chunks
    """
    client = await get_async_openai_client()
    
    # Create a run to execute the assistant
    run_params = {
        "thread_id": thread_id,
        "assistant_id": assistant_id
    }
    
    if run_instructions:
        run_params["instructions"] = run_instructions
    
    # Create and execute the run
    run = await client.beta.threads.runs.create(**run_params)
    
    # Poll for run completion
    while True:
        run = await client.beta.threads.runs.retrieve(
            thread_id=thread_id,
            run_id=run.id
        )
        
        if run.status == "completed":
            break
        elif run.status in ["failed", "cancelled", "expired"]:
            raise Exception(f"Run ended with status: {run.status}, {run.last_error}")
        
        # Yield a progress update
        yield {"status": run.status, "type": "status_update"}
        
        # Wait before polling again
        await asyncio.sleep(1)
    
    # Get the assistant's messages (only the new ones created after the run)
    messages = await client.beta.threads.messages.list(
        thread_id=thread_id,
        order="desc",
        limit=5
    )
    
    # Get the most recent assistant message (should be the response to the user's message)
    for message in messages.data:
        if message.role == "assistant":
            # For text content, stream it
            for content_part in message.content:
                if content_part.type == "text":
                    # Stream the text content in chunks to simulate streaming
                    text = content_part.text.value
                    # Split the content into chunks (simulating streaming)
                    chunk_size = 10  # characters per chunk
                    for i in range(0, len(text), chunk_size):
                        chunk = text[i:i+chunk_size]
                        yield {
                            "content": chunk,
                            "type": "content_chunk"
                        }
                        await asyncio.sleep(0.02)  # Slight delay for realistic streaming
                    
                    # Signal end of streaming
                    yield {
                        "done": True,
                        "type": "completion"
                    }
                    
                    # Only process the first assistant message
                    return
    
    # If no message was found
    yield {
        "content": "No response generated by the assistant.",
        "done": True,
        "type": "error"
    }


async def enhanced_stream_openai_chat(messages, tool=None, image_url=None):
    """
    Enhanced version of the standard chat completions API with improved streaming.
    Instead of sending the entire accumulated response on each token, it only sends the new tokens.
    
    Args:
        messages: List of message objects (role, content)
        tool: AITool model instance (for parameters)
        image_url: Optional base64 image URL
        
    Returns:
        Async generator of completion chunks with token and accumulated content
    """
    api_version = os.environ.get('AZURE_OPENAI_API_VERSION')
    deployment = os.environ.get('AZURE_OPENAI_DEPLOYMENT')
    
    # Get parameters from tool or use defaults
    max_tokens = getattr(tool, 'max_completion_tokens', 800)
    temperature = getattr(tool, 'temperature', 1.0)
    top_p = getattr(tool, 'top_p', 1.0)
    frequency_penalty = getattr(tool, 'frequency_penalty', 0.0)
    presence_penalty = getattr(tool, 'presence_penalty', 0.0)
    
    # Handle image input if present
    if image_url and image_url.startswith('data:image'):
        # Extract the last user message
        user_message = messages[-1]['content'] if messages[-1]['role'] == 'user' else ""
        
        # Create multimodal content
        input_content = [
            {"type": "text", "text": user_message},
            {
                "type": "image_url",
                "image_url": {"url": image_url, "detail": "auto"}
            }
        ]
        
        # Reconstruct messages with the multimodal content
        messages = messages[:-1] + [{"role": "user", "content": input_content}]
    
    async with await get_async_openai_client() as client:
        stream = await client.chat.completions.create(
            model=deployment,
            messages=messages,
            stream=True,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            frequency_penalty=frequency_penalty,
            presence_penalty=presence_penalty
        )
        
        current_content = ""
        
        async for chunk in stream:
            if hasattr(chunk, "choices") and chunk.choices:
                delta = chunk.choices[0].delta
                
                # Check for content in the delta
                if hasattr(delta, "content") and delta.content:
                    # Extract the new token
                    new_content = delta.content
                    current_content += new_content
                    
                    # Yield only the new content and a done flag
                    yield {
                        "content": new_content,  # Just the new token(s)
                        "done": False,
                        "finish_reason": None,
                        "type": "content_chunk"
                    }
                
                # Check for finish reason
                if chunk.choices[0].finish_reason:
                    yield {
                        "content": "",
                        "done": True,
                        "finish_reason": chunk.choices[0].finish_reason,
                        "type": "completion"
                    }
        
        # Signal completion if not already done
        yield {
            "content": "",
            "done": True,
            "finish_reason": "stop",
            "type": "completion"
        }


@database_sync_to_async
def prepare_openai_thread(conversation, tool):
    """
    Prepare an OpenAI thread for the conversation. Creates the thread if not exists.
    
    Args:
        conversation: The Conversation model instance
        tool: The AITool model instance
        
    Returns:
        thread_id: OpenAI Thread ID
    """
    return get_or_create_thread(conversation)


@database_sync_to_async
def add_user_message_to_thread(thread_id, user_message, files=None):
    """
    Add a user message to the OpenAI thread.
    
    Args:
        thread_id: OpenAI Thread ID
        user_message: User message content
        files: Optional list of OpenAI File IDs
        
    Returns:
        Message object
    """
    return add_message_to_thread(thread_id, user_message, files, is_user=True)


@database_sync_to_async
def add_assistant_message_to_thread(thread_id, assistant_message):
    """
    Add an assistant message to the OpenAI thread.
    
    Args:
        thread_id: OpenAI Thread ID
        assistant_message: Assistant message content
        
    Returns:
        Message object
    """
    return add_message_to_thread(thread_id, assistant_message, is_user=False)


@database_sync_to_async
def get_associated_files(message_id):
    """
    Get OpenAI File IDs associated with a message.
    
    Args:
        message_id: Message ID
        
    Returns:
        List of OpenAI File IDs
    """
    message_files = MessageOpenAIFile.objects.filter(message_id=message_id)
    file_ids = [mf.openai_file.openai_file_id for mf in message_files]
    return file_ids
